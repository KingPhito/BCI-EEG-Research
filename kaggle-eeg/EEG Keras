{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EEG Keras","provenance":[],"authorship_tag":"ABX9TyNJ8aQh8YOJPg7rgKkhVC82"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"grfgFhsOv_O4","colab_type":"text"},"source":["# **EEG Model with Keras and Wandb**\n","This is a test project I am using to learn Keras for structured data. I am using data from a past Kaggle competition to train a model that can detect certain events from EEG brainwave data. The events would then trigger certain gestures in a prosthetic device for example, using BCI technology. My goal is to get perfect/near perfect predictions on the testing data. You can get more info on the contest/dataset [here](https://www.kaggle.com/c/grasp-and-lift-eeg-detection/)"]},{"cell_type":"markdown","metadata":{"id":"S1S32XeQzYh9","colab_type":"text"},"source":["## **Install The Libraries**\n","First we install install all necessary Python libraries with pip."]},{"cell_type":"code","metadata":{"id":"z0LXxf1231d1","colab_type":"code","colab":{}},"source":["!pip install sklearn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgcWF5SL8T-o","colab_type":"code","colab":{}},"source":["!pip install wandb\n","!pip install kaggle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6PUd-ug2jNC","colab_type":"text"},"source":["## **Kaggle Environment Setup**\n","You will need to upload your *kaggle.json*, set the permissions so the file can be read."]},{"cell_type":"code","metadata":{"id":"f0UnRzaJWuBG","colab_type":"code","colab":{}},"source":["!chmod 600 ./kaggle.json"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VmBsLfY0OS39","colab_type":"text"},"source":["Then we set the Kaggle configuration directory to our current working directory, as an environment variable."]},{"cell_type":"code","metadata":{"id":"Zge9yawNW31i","colab_type":"code","colab":{}},"source":["import os\n","os.environ['KAGGLE_CONFIG_DIR'] = '.'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kZmR-eNASVQ","colab_type":"text"},"source":["Now we can download the data from the competition page, "]},{"cell_type":"code","metadata":{"id":"pWytfHWCkA_C","colab_type":"code","colab":{}},"source":["!kaggle competitions download grasp-and-lift-eeg-detection -f train.zip"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSj3R5FGNp8w","colab_type":"text"},"source":["and unzip it into the data directory."]},{"cell_type":"code","metadata":{"id":"fJja6QKfmt7D","colab_type":"code","colab":{}},"source":["!unzip train.zip -d data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcKFL0C3AnUD","colab_type":"text"},"source":["## **Data Setup**\n","First let's import all the libraries we need."]},{"cell_type":"code","metadata":{"id":"TAtBsA0TAx5n","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","\n","from tensorflow import feature_column\n","from tensorflow.keras import layers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.backend import categorical_crossentropy\n","from sklearn.preprocessing import LabelEncoder"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1ePPH8NS3ck","colab_type":"text"},"source":["#### **Pandas**"]},{"cell_type":"markdown","metadata":{"id":"LqsEMWAJCtPu","colab_type":"text"},"source":["Next we specify the path of our training data. Then we're specifying the types of our features and labels, because otherwise pandas will use a lot of memory storing them. \n","\n","The training data is separated into files representing 12 test subjects and 8 series per subject. And the files with our labels have a suffix of `events` attached to them. \n","* The `load_one_series` function takes a specific series of a subject, and merges it with the labels from the corresponding `events` csv.\n","* The `get_merged_series` function takes the nth series(the function parameter) \n","from every subject, and merges them into one dataframe using the `load_one_series` function. We're going to use this for our validation set."]},{"cell_type":"code","metadata":{"id":"ZEYW4xziOqSq","colab_type":"code","colab":{}},"source":["path = './data/train'\n","\n","feature_types = {\n","    'Id': 'str', 'Fp1': 'int16', 'Fp2': 'int16', 'F7': 'int16', 'F3': 'int16', 'Fz': 'int16',\n","    'F4': 'int16', 'F8': 'int16', 'FC5': 'int16', 'FC1': 'int16', 'FC2': 'int16', 'FC6': 'int16',\n","    'T7': 'int16', 'C3': 'int16', 'Cz': 'int16', 'C4': 'int16', 'T8': 'int16', 'TP9': 'int16',\n","    'CP5': 'int16', 'CP1': 'int16', 'CP2': 'int16', 'CP6': 'int16', 'TP10': 'int16', 'P7': 'int16',\n","    'P3': 'int16', 'Pz': 'int16', 'P4': 'int16', 'P8': 'int16', 'PO9': 'int16', 'O1': 'int16',\n","    'Oz': 'int16', 'O2': 'int16', 'PO10': 'int16'\n","}\n","\n","label_types = {\n","    'Id': 'str', 'HandStart': 'int8', 'FirstDigitTouch': 'int8', 'LiftOff': 'int8', \n","    'Replace': 'int8', 'BothReleased': 'int8', 'BothStartLoadPhase': 'int8'\n","}\n","\n","def load_one_series(sj, sr):\n","  df = pd.read_csv(f'{path}/subj{sj}_series{sr}_data.csv', dtype=feature_types, encoding='utf8')\n","  labels = pd.read_csv(f'{path}/subj{sj}_series{sr}_events.csv', dtype=label_types)\n","  keys = labels.keys()\n","  for id in range(0, len(df.index)):\n","    df.loc[id, 'Labels'] = 'None'\n","    nl = '\\n'\n","    log = f\"Rows left to label: {len(df.index) - id}...{nl}\"\n","    print(log)\n","    for col in keys:\n","      if labels.at[id, col] == 1:\n","        df.loc[id, 'Labels'] = col\n","        break\n","  return df\n","\n","def get_merged_series(sr):\n","  df = None\n","  for sj in range(1, 13):\n","    temp = load_one_series(sj, sr)\n","    df = temp if df is None else df.append(temp)\n","  return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMuGje4AFIel","colab_type":"text"},"source":["Creating the validation set will take some time as these files are quite large. Make some coffee =). But once it's done we can save it locally to load faster in the future. So you should only need to do this once.  "]},{"cell_type":"code","metadata":{"id":"yt_jygH1z6kD","colab_type":"code","colab":{}},"source":["valid = get_merged_series(8)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_T7Qq8HggFc","colab_type":"code","colab":{}},"source":["valid.to_csv('valid.csv', index=False, float_format='%.4f')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7YWNPjXYEPm1","colab_type":"text"},"source":["Here we load in our validation data."]},{"cell_type":"code","metadata":{"id":"KU0KJJoJg_Yq","colab_type":"code","colab":{}},"source":["valid = pd.read_csv('valid.csv', encoding='utf8')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WFg3jfKKIN0","colab_type":"text"},"source":["Then we generate our first training set."]},{"cell_type":"code","metadata":{"id":"u-7g1VyBJAxU","colab_type":"code","colab":{}},"source":["train = load_one_series(1, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRVdUjEYnR8G","colab_type":"text"},"source":["The following two lines just remove the `id` column from our dataframes since we don't need them."]},{"cell_type":"code","metadata":{"id":"k8Hw7DfNmavY","colab_type":"code","colab":{}},"source":["train = train.loc[:, ~train.columns.str.match('id')]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fR1-ImVfmmen","colab_type":"code","colab":{}},"source":["valid = valid.loc[:, ~valid.columns.str.match('id')]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hs9UXHoBsGVx","colab_type":"text"},"source":["We can check here to make sure our data is organized as expected."]},{"cell_type":"code","metadata":{"id":"VONkhLnTpWT8","colab_type":"code","outputId":"efcde749-811c-4e83-dad0-f1ce15f325f2","executionInfo":{"status":"ok","timestamp":1590178519359,"user_tz":240,"elapsed":409,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fp1</th>\n","      <th>Fp2</th>\n","      <th>F7</th>\n","      <th>F3</th>\n","      <th>Fz</th>\n","      <th>F4</th>\n","      <th>F8</th>\n","      <th>FC5</th>\n","      <th>FC1</th>\n","      <th>FC2</th>\n","      <th>FC6</th>\n","      <th>T7</th>\n","      <th>C3</th>\n","      <th>Cz</th>\n","      <th>C4</th>\n","      <th>T8</th>\n","      <th>TP9</th>\n","      <th>CP5</th>\n","      <th>CP1</th>\n","      <th>CP2</th>\n","      <th>CP6</th>\n","      <th>TP10</th>\n","      <th>P7</th>\n","      <th>P3</th>\n","      <th>Pz</th>\n","      <th>P4</th>\n","      <th>P8</th>\n","      <th>PO9</th>\n","      <th>O1</th>\n","      <th>Oz</th>\n","      <th>O2</th>\n","      <th>PO10</th>\n","      <th>Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-31</td>\n","      <td>363</td>\n","      <td>211</td>\n","      <td>121</td>\n","      <td>211</td>\n","      <td>15</td>\n","      <td>717</td>\n","      <td>279</td>\n","      <td>35</td>\n","      <td>158</td>\n","      <td>543</td>\n","      <td>-166</td>\n","      <td>192</td>\n","      <td>230</td>\n","      <td>573</td>\n","      <td>860</td>\n","      <td>128</td>\n","      <td>59</td>\n","      <td>272</td>\n","      <td>473</td>\n","      <td>325</td>\n","      <td>379</td>\n","      <td>536</td>\n","      <td>348</td>\n","      <td>383</td>\n","      <td>105</td>\n","      <td>607</td>\n","      <td>289</td>\n","      <td>459</td>\n","      <td>173</td>\n","      <td>120</td>\n","      <td>704</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-29</td>\n","      <td>342</td>\n","      <td>216</td>\n","      <td>123</td>\n","      <td>222</td>\n","      <td>200</td>\n","      <td>595</td>\n","      <td>329</td>\n","      <td>43</td>\n","      <td>166</td>\n","      <td>495</td>\n","      <td>-138</td>\n","      <td>201</td>\n","      <td>233</td>\n","      <td>554</td>\n","      <td>846</td>\n","      <td>185</td>\n","      <td>47</td>\n","      <td>269</td>\n","      <td>455</td>\n","      <td>307</td>\n","      <td>368</td>\n","      <td>529</td>\n","      <td>327</td>\n","      <td>369</td>\n","      <td>78</td>\n","      <td>613</td>\n","      <td>248</td>\n","      <td>409</td>\n","      <td>141</td>\n","      <td>83</td>\n","      <td>737</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-172</td>\n","      <td>278</td>\n","      <td>105</td>\n","      <td>93</td>\n","      <td>222</td>\n","      <td>511</td>\n","      <td>471</td>\n","      <td>280</td>\n","      <td>12</td>\n","      <td>177</td>\n","      <td>534</td>\n","      <td>-163</td>\n","      <td>198</td>\n","      <td>207</td>\n","      <td>542</td>\n","      <td>768</td>\n","      <td>145</td>\n","      <td>52</td>\n","      <td>250</td>\n","      <td>452</td>\n","      <td>273</td>\n","      <td>273</td>\n","      <td>511</td>\n","      <td>319</td>\n","      <td>355</td>\n","      <td>66</td>\n","      <td>606</td>\n","      <td>320</td>\n","      <td>440</td>\n","      <td>141</td>\n","      <td>62</td>\n","      <td>677</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-272</td>\n","      <td>263</td>\n","      <td>-52</td>\n","      <td>99</td>\n","      <td>208</td>\n","      <td>511</td>\n","      <td>428</td>\n","      <td>261</td>\n","      <td>27</td>\n","      <td>180</td>\n","      <td>525</td>\n","      <td>-310</td>\n","      <td>212</td>\n","      <td>221</td>\n","      <td>542</td>\n","      <td>808</td>\n","      <td>115</td>\n","      <td>41</td>\n","      <td>276</td>\n","      <td>432</td>\n","      <td>258</td>\n","      <td>241</td>\n","      <td>521</td>\n","      <td>336</td>\n","      <td>356</td>\n","      <td>71</td>\n","      <td>568</td>\n","      <td>339</td>\n","      <td>437</td>\n","      <td>139</td>\n","      <td>58</td>\n","      <td>592</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-265</td>\n","      <td>213</td>\n","      <td>-67</td>\n","      <td>99</td>\n","      <td>155</td>\n","      <td>380</td>\n","      <td>476</td>\n","      <td>353</td>\n","      <td>32</td>\n","      <td>165</td>\n","      <td>507</td>\n","      <td>-320</td>\n","      <td>242</td>\n","      <td>230</td>\n","      <td>545</td>\n","      <td>865</td>\n","      <td>180</td>\n","      <td>89</td>\n","      <td>288</td>\n","      <td>444</td>\n","      <td>275</td>\n","      <td>275</td>\n","      <td>550</td>\n","      <td>324</td>\n","      <td>346</td>\n","      <td>76</td>\n","      <td>547</td>\n","      <td>343</td>\n","      <td>446</td>\n","      <td>171</td>\n","      <td>67</td>\n","      <td>581</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Fp1  Fp2   F7   F3   Fz   F4   F8  ...   P8  PO9   O1   Oz   O2  PO10  Labels\n","0  -31  363  211  121  211   15  717  ...  607  289  459  173  120   704    None\n","1  -29  342  216  123  222  200  595  ...  613  248  409  141   83   737    None\n","2 -172  278  105   93  222  511  471  ...  606  320  440  141   62   677    None\n","3 -272  263  -52   99  208  511  428  ...  568  339  437  139   58   592    None\n","4 -265  213  -67   99  155  380  476  ...  547  343  446  171   67   581    None\n","\n","[5 rows x 33 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"unb8Q61t-OE8","colab_type":"code","colab":{}},"source":["valid.info(verbose=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"432assJUFCp6","colab_type":"text"},"source":["### **Tensorflow Data Loading**\n","Here, we will create [tf.data](https://www.tensorflow.org/guide/datasets) with our pandas dataframes above. For this we will borrow a function from the Tensorflow structured data tutorial, and slightly modify it to convert our string labels to integer values."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NkcaMYP-MsRe","colab":{}},"source":["actions = { 'None': 0, 'HandStart': 1, 'FirstDigitTouch': 2, 'LiftOff': 3, \n","    'Replace': 4, 'BothReleased': 5, 'BothStartLoadPhase': 6 }\n","# A utility method to create a tf.data dataset from a Pandas Dataframe\n","def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n","  dataframe = dataframe.copy()\n","  labels = dataframe.pop('Labels')\n","  label_cats = [actions[k] for k in labels]\n","  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), label_cats))\n","  if shuffle:\n","    ds = ds.shuffle(buffer_size=len(dataframe))\n","  ds = ds.batch(batch_size)\n","  return ds"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lQqif4-rR0P","colab_type":"text"},"source":["Now we create training and validation sets."]},{"cell_type":"code","metadata":{"id":"fhopYs5bLQ-A","colab_type":"code","colab":{}},"source":["train_ds = df_to_dataset(train, batch_size=64)\n","val_ds = df_to_dataset(valid, shuffle=False, batch_size=128)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgX5tgwTrd1_","colab_type":"text"},"source":["We can examine a training batch here to make sure everything is looking right."]},{"cell_type":"code","metadata":{"id":"7P3WopVB7OYy","colab_type":"code","outputId":"0eaf14f8-5008-48ad-ae1f-977319799684","executionInfo":{"status":"ok","timestamp":1590041472551,"user_tz":240,"elapsed":2233,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["for feature_batch, label_batch in train_ds.take(1):\n","  print('Every feature:', list(feature_batch.keys()))\n","  print('A batch of Fp1:', feature_batch['Fp1'])\n","  print('A batch of targets:', label_batch )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Every feature: ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6', 'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10', 'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz', 'O2', 'PO10']\n","A batch of Fp1: tf.Tensor(\n","[  14   61  296  434 -175  440  605  325 1112   92  508  -61  -67   48\n","  736  361  412  450   87  -82  478   59  -96  292  142 -148  269   54\n"," 2076  195  786  132  658  521  456  159  185 -144 -324  231  370  152\n","  208   87  -31  -73 1404  213  348  286   99   57  -60 -236   25  543\n"," 1536 -112  275  392   31  242 -153  392], shape=(64,), dtype=int16)\n","A batch of targets: tf.Tensor(\n","[4 1 3 0 4 0 0 5 0 0 0 0 0 0 0 0 0 0 0 5 0 4 0 0 1 0 0 0 0 0 2 5 0 0 0 0 0\n"," 0 0 0 5 0 0 3 0 0 0 3 0 0 0 0 4 0 0 3 0 1 0 0 1 0 0 0], shape=(64,), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4V1PII80TV62","colab_type":"text"},"source":["## **Training**"]},{"cell_type":"markdown","metadata":{"id":"_c1Y6FDmXONd","colab_type":"text"},"source":["### **Wandb Logging**\n","First we're going to login to Wandb with our api key so that we can log the training. "]},{"cell_type":"code","metadata":{"id":"HT17aFKIZhU3","colab_type":"code","outputId":"648d8ffe-b8af-426b-cd13-2c8485a4fa12","executionInfo":{"status":"ok","timestamp":1590177048979,"user_tz":240,"elapsed":1953,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!wandb login d754544ba90d0be7ea7009afb39a9225330e6be9"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZnHT5d7sQD5E","colab_type":"text"},"source":["Initialize Wandb and specify a project name to keep track of metrics"]},{"cell_type":"code","metadata":{"id":"EA-GduHoOV1k","colab_type":"code","outputId":"7ee58405-29af-4ada-fe2b-9402e4aad352","executionInfo":{"status":"ok","timestamp":1590177055643,"user_tz":240,"elapsed":3948,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import wandb\n","from wandb.keras import WandbCallback\n","\n","wandb.init(project=\"kaggle-eeg-keras\", config={\"hyper\": \"parameter\"})"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://app.wandb.ai/rdugue/kaggle-eeg-keras\" target=\"_blank\">https://app.wandb.ai/rdugue/kaggle-eeg-keras</a><br/>\n","                Run page: <a href=\"https://app.wandb.ai/rdugue/kaggle-eeg-keras/runs/2by9jg5c\" target=\"_blank\">https://app.wandb.ai/rdugue/kaggle-eeg-keras/runs/2by9jg5c</a><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["W&B Run: https://app.wandb.ai/rdugue/kaggle-eeg-keras/runs/2by9jg5c"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"ZXdPEyThXapx","colab_type":"text"},"source":["### **Model**\n","Here we are creating our feature layer from the feature column headers in our dataset."]},{"cell_type":"code","metadata":{"id":"Z1E61q2RQseS","colab_type":"code","colab":{}},"source":["cont_vars = [\n","    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2',\n","    'FC6', 'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2',\n","    'CP6', 'TP10', 'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz',\n","    'O2', 'PO10'\n","]\n","\n","feature_columns = []\n","\n","for header in cont_vars:\n","  feature_columns.append(feature_column.numeric_column(header))\n","\n","feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ARZS6sugr-gt","colab_type":"text"},"source":["Now we can create our Keras model for training."]},{"cell_type":"code","metadata":{"id":"SLDhXu7cUuzr","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","  feature_layer,\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(1, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gw_Ee3SEkcYx","colab_type":"text"},"source":["### **Fitting**\n","This is our training loop."]},{"cell_type":"code","metadata":{"id":"dsC-1lGlaNoQ","colab_type":"code","colab":{}},"source":["def train_loop():\n","  model.fit(train_ds, validation_data=val_ds, epochs=2, callbacks=[WandbCallback()])\n","  for sr in range(1, 8):\n","    for sj in range(2, 13):\n","      train = load_one_series(sj, sr)\n","      model.fit(df_to_dataset(train, batch_size=64), validation_data=val_ds, epochs=2, callbacks=[WandbCallback()])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tH-brjPOrJb6","colab_type":"code","colab":{}},"source":["train_loop()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2K--Jt6182Ze","colab_type":"text"},"source":["## **Testing**\n","We're gonna download the testing data now from the Kaggle competition and unzip into the data directory."]},{"cell_type":"code","metadata":{"id":"Mcw0qbEpqkjI","colab_type":"code","colab":{}},"source":["!kaggle competitions download grasp-and-lift-eeg-detection -f test.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uF-4ZsA3Ia_C","colab_type":"code","colab":{}},"source":["!unzip test.zip -d data/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDLUFWKonI0l","colab_type":"text"},"source":["Here we load the sample submission from the Kaggle competition. This gives us a prem-made dataframe and we just need to update column values with predictions from our model. "]},{"cell_type":"code","metadata":{"id":"MPndLKOSVOR5","colab_type":"code","colab":{}},"source":["sub = pd.read_csv('sample_submission.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPV2V3mHnPSH","colab_type":"code","outputId":"0d5eac3c-cf12-4cb8-8e18-5eaeab35d98b","executionInfo":{"status":"ok","timestamp":1590182742782,"user_tz":240,"elapsed":263,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["sub.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>HandStart</th>\n","      <th>FirstDigitTouch</th>\n","      <th>BothStartLoadPhase</th>\n","      <th>LiftOff</th>\n","      <th>Replace</th>\n","      <th>BothReleased</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>subj1_series9_0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>subj1_series9_1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>subj1_series9_2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>subj1_series9_3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>subj1_series9_4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                id  HandStart  FirstDigitTouch  ...  LiftOff  Replace  BothReleased\n","0  subj1_series9_0          0                0  ...        0        0             0\n","1  subj1_series9_1          0                0  ...        0        0             0\n","2  subj1_series9_2          0                0  ...        0        0             0\n","3  subj1_series9_3          0                0  ...        0        0             0\n","4  subj1_series9_4          0                0  ...        0        0             0\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"VFNIfCiJ-Edc","colab_type":"text"},"source":["Here we create a dataframe in the same shape as the example submission on the competition page."]},{"cell_type":"code","metadata":{"id":"u5Dic5uwJZS3","colab_type":"code","colab":{}},"source":["path = './data/test'\n","\n","def get_merged_tests():\n","  tests = None\n","  for sj in range(1, 13):\n","    for sr in range(9, 11):\n","      c_tests = pd.read_csv(f'{path}/subj{sj}_series{sr}_data.csv', dtype=feature_types)\n","      tests = c_tests if tests is None else tests.append(c_tests, ignore_index=True)\n","  return tests"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pL6TXJ-Th65c","colab_type":"code","colab":{}},"source":["tests = get_merged_tests()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Og5BNjrQmsY4","colab":{}},"source":["tests = tests.loc[:, ~tests.columns.str.match('id')]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vyBggI2H2W8v","colab_type":"text"},"source":["Keras has a weird bug that won't allow us to load weights from a previous session unless it's been built. So we do a quick 1 epoch here to satisfy that requirement."]},{"cell_type":"code","metadata":{"id":"G2iLH7LeDP-5","colab_type":"code","outputId":"f1fcf2d0-188a-4f1f-971e-10421cae0538","executionInfo":{"status":"ok","timestamp":1590291179975,"user_tz":240,"elapsed":48458,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["model.fit(train_ds, validation_data=val_ds, epochs=1)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["1868/1868 [==============================] - 46s 25ms/step - loss: 0.0000e+00 - accuracy: 0.0427 - val_loss: 0.0000e+00 - val_accuracy: 0.0388\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1721d3b940>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"FRJ4pf5BhH2b","colab_type":"code","colab":{}},"source":["model.load_weights('model-best.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt1SFkuWqn9q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":100},"outputId":"1ae1fe8d-f846-4590-cc16-3dfdee982c7d","executionInfo":{"status":"ok","timestamp":1590291396493,"user_tz":240,"elapsed":377,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}}},"source":["out = tests.loc[[0], :]  \n","out.head()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Fp1</th>\n","      <th>Fp2</th>\n","      <th>F7</th>\n","      <th>F3</th>\n","      <th>Fz</th>\n","      <th>F4</th>\n","      <th>F8</th>\n","      <th>FC5</th>\n","      <th>FC1</th>\n","      <th>FC2</th>\n","      <th>FC6</th>\n","      <th>T7</th>\n","      <th>C3</th>\n","      <th>Cz</th>\n","      <th>C4</th>\n","      <th>T8</th>\n","      <th>TP9</th>\n","      <th>CP5</th>\n","      <th>CP1</th>\n","      <th>CP2</th>\n","      <th>CP6</th>\n","      <th>TP10</th>\n","      <th>P7</th>\n","      <th>P3</th>\n","      <th>Pz</th>\n","      <th>P4</th>\n","      <th>P8</th>\n","      <th>PO9</th>\n","      <th>O1</th>\n","      <th>Oz</th>\n","      <th>O2</th>\n","      <th>PO10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>66</td>\n","      <td>213</td>\n","      <td>347</td>\n","      <td>212</td>\n","      <td>-102</td>\n","      <td>361</td>\n","      <td>244</td>\n","      <td>293</td>\n","      <td>52</td>\n","      <td>27</td>\n","      <td>337</td>\n","      <td>110</td>\n","      <td>329</td>\n","      <td>-48</td>\n","      <td>241</td>\n","      <td>114</td>\n","      <td>-145</td>\n","      <td>50</td>\n","      <td>56</td>\n","      <td>106</td>\n","      <td>241</td>\n","      <td>-170</td>\n","      <td>327</td>\n","      <td>113</td>\n","      <td>15</td>\n","      <td>262</td>\n","      <td>449</td>\n","      <td>-118</td>\n","      <td>-25</td>\n","      <td>88</td>\n","      <td>10</td>\n","      <td>119</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Fp1  Fp2   F7   F3   Fz   F4   F8  FC5  ...  Pz   P4   P8  PO9  O1  Oz  O2  PO10\n","0   66  213  347  212 -102  361  244  293  ...  15  262  449 -118 -25  88  10   119\n","\n","[1 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"5qoR7dLqMbtI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":683},"outputId":"e7ce69f3-dab7-4943-d34f-399ed15132a5","executionInfo":{"status":"error","timestamp":1590291332097,"user_tz":240,"elapsed":20236,"user":{"displayName":"Ralph Dugue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GipnJHGOp2_crugLbpCgfBA1Eg6JMAO-u2DmIQdRg=s64","userId":"10136421352402968883"}}},"source":["np.argmax(model.predict(out.to_dict()), axis=-1)"],"execution_count":31,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-0711fb575dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1247\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 963\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    964\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     raise RuntimeError(\n","\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {'(<class \\'dict\\'> containing {\"<class \\'int\\'>\"} keys and {\"<class \\'int\\'>\"} values)'} values), <class 'NoneType'>"]}]},{"cell_type":"code","metadata":{"id":"dI8MZByMhTm-","colab_type":"code","colab":{}},"source":["classes = actions.keys()\n","for id in range(0, len(tests.index)):\n","    pred = classes[model.predict(tests.loc[id])[1]]\n","    nl = '\\n'\n","    log = f\"Current pred: {pred}. Rows left to predict: {len(tests.index) - id}...{nl}\"\n","    print(log)\n","    for col in sub.keys():\n","      if col == pred:\n","        sub.loc[id][col] = 1\n","      else:\n","        sub.loc[id][col] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMACdgjbpxTf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}